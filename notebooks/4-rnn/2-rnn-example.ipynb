{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv is True :\n",
    "        return x*(1-x)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid(layer_2, True))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid(layer_1, True)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing our dependencies and seeding the random number generator.  \n",
    "* Numpy is for matrix algebra\n",
    "* Copy is to copy things\n",
    "```python\n",
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "```\n",
    "\n",
    "\n",
    "Our nonlinearity and derivative.\n",
    "```python\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv is True :\n",
    "        return x*(1-x)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "We're going to create a lookup table that maps from an integer to its binary representation.  \n",
    "The binary representations will be our input and output data for each math problem we try to get the network to solve.  \n",
    "This lookup table will be very helpful in converting from integers to bit strings.\n",
    "```python\n",
    "int2binary = {}\n",
    "```\n",
    "\n",
    "This is where I set the maximum length of the binary numbers we'll be adding.  \n",
    "If I've done everything right, you can adjust this to add potentially very large numbers.\n",
    "```python\n",
    "binary_dim = 8\n",
    "```\n",
    "\n",
    "This computes the largest number that is possible to represent with the binary length we chose\n",
    "```python\n",
    "largest_number = pow(2,binary_dim)\n",
    "```\n",
    "\n",
    "This is a lookup table that maps from an integer to its binary representation.  \n",
    "We copy it into the int2binary. This is kindof un-ncessary but I thought it made things more obvious looking.\n",
    "```python\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T, axis=1)\n",
    "\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "```\n",
    "\n",
    "This is our learning rate.\n",
    "```python\n",
    "alpha = 0.1\n",
    "```\n",
    "\n",
    "We are adding two numbers together, so we'll be feeding in two-bit strings one character at the time each.  \n",
    "Thus, we need to have two inputs to the network (one for each of the numbers being added).\n",
    "```python\n",
    "input_dim = 2\n",
    "```\n",
    "\n",
    "This is the size of the hidden layer that will be storing our carry bit.  \n",
    "Notice that it is way larger than it theoretically needs to be.  \n",
    "Play with this and see how it affects the speed of convergence.  \n",
    "* Do larger hidden dimensions make things train faster or slower? \n",
    "* More iterations or fewer?\n",
    "```python\n",
    "hidden_dim = 16\n",
    "```\n",
    "\n",
    "Well, we're only predicting the sum, which is one number. Thus, we only need one output\n",
    "```python\n",
    "output_dim = 1\n",
    "```\n",
    "\n",
    "This is the matrix of weights that connects our input layer and our hidden layer.  \n",
    "Thus, it has \"input_dim\" rows and \"hidden_dim\" columns. (2 x 16 unless you change it).  \n",
    "```python\n",
    "synapse_0 = 2*np.random.random((input_dim, hidden_dim)) - 1\n",
    "```\n",
    "\n",
    "This is the matrix of weights that connects the hidden layer to the output layer. \n",
    "Thus, it has \"hidden_dim\" rows and \"output_dim\" columns. (16 x 1 unless you change it). \n",
    "```python\n",
    "synapse_1 = 2*np.random.random((hidden_dim, output_dim)) - 1\n",
    "```\n",
    "\n",
    "This is the matrix of weights that connects the hidden layer in the previous time-step to the hidden layer in the current timestep. It also connects the hidden layer in the current timestep to the hidden layer in the next timestep (we keep using it).  \n",
    "Thus, it has the dimensionality of \"hidden_dim\" rows and \"hidden_dim\" columns. (16 x 16 unless you change it). \n",
    "```python\n",
    "synapse_h = 2*np.random.random((hidden_dim, hidden_dim)) - 1\n",
    "```\n",
    "\n",
    "These store the weight updates that we would like to make for each of the weight matrices.  \n",
    "After we've accumulated several weight updates, we'll actually update the matrices.\n",
    "```python\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "```\n",
    "\n",
    "We're iterating over 10,000 training examples\n",
    "```python\n",
    "for j in range(10000):\n",
    "    ...\n",
    "```\n",
    "\n",
    "We're going to generate a random addition problem. So, we're initializing an integer randomly between 0 and half of the largest value we can represent. If we allowed the network to represent more than this, than adding two number could theoretically overflow (be a bigger number than we have bits to represent).  \n",
    "Thus, we only add numbers that are less than half of the largest number we can represent.\n",
    "```python\n",
    "a_int = np.random.randint(largest_number/2) # int version\n",
    "```\n",
    "\n",
    "We lookup the binary form for \"a_int\" and store it in \"a\"\n",
    "```python\n",
    "a = int2binary[a_int] # binary encoding\n",
    "```\n",
    "\n",
    "Same as \"a_int\", just getting another random number.\n",
    "```python\n",
    "b_int = np.random.randint(largest_number/2) # int version\n",
    "```\n",
    "\n",
    "Same as \"a\", looking up the binary representation.\n",
    "```python\n",
    "b = int2binary[b_int] # binary encoding\n",
    "```\n",
    "\n",
    "We're computing what the correct answer should be for this addition\n",
    "```python\n",
    "c_int = a_int + b_int\n",
    "```\n",
    "\n",
    "Converting the true answer to its binary representation\n",
    "```python\n",
    "c = int2binary[c_int]\n",
    "```\n",
    "\n",
    "Initializing an empty binary array where we'll store the neural network's predictions (so we can see it at the end).  \n",
    "You could get around doing this if you want...but i thought it made things more intuitive\n",
    "```python\n",
    "# where we'll store our best guess (binary encoded)\n",
    "d = np.zeros_like(c)\n",
    "```\n",
    "\n",
    "Resetting the error measure\n",
    "```python\n",
    "overallError = 0\n",
    "```\n",
    "\n",
    "These two lists will keep track of the layer 2 derivatives and layer 1 values at each time step.\n",
    "```python\n",
    "layer_2_deltas = list()\n",
    "layer_1_values = list()\n",
    "```\n",
    "\n",
    "Time step zero has no previous hidden layer, so we initialize one that's off.\n",
    "```python\n",
    "layer_1_values.append(np.zeros(hidden_dim))\n",
    "```\n",
    "\n",
    "#### forward propagation\n",
    "This for loop iterates through the binary representation\n",
    "```python\n",
    "for position in range(binary_dim):\n",
    "    ...\n",
    "```\n",
    "\n",
    "X is a list of 2 numbers, one from a and one from b. It's indexed according to the \"position\" variable, but we index it in such a way that it goes from right to left. So, when position == 0, this is the farhest bit to the right in \"a\" and the farthest bit to the right in \"b\". When position equals 1, this shifts to the left one bit. \n",
    "```python\n",
    "X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n",
    "```\n",
    "\n",
    "Same indexing as `layer_1_values.append(np.zeros(hidden_dim))`, but instead it's the value of the correct answer (either a 1 or a 0)\n",
    "```python\n",
    "y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "```\n",
    "\n",
    "**This is the magic!!!** To construct the hidden layer, we first do two things. \n",
    "* First, we propagate from the input to the hidden layer `np.dot(X,synapse_0)`. \n",
    "* Then, we propagate from the previous hidden layer to the current hidden layer `np.dot(prev_layer_1, synapse_h)`. \n",
    "* Then WE SUM THESE TWO VECTORS!!!!... \n",
    "* and pass through the sigmoid function.\n",
    "\n",
    "So, how do we combine the information from the previous hidden layer and the input? After each has been propagated through its various matrices (read: interpretations), we sum the information. \n",
    "```python\n",
    "layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "```\n",
    "\n",
    "It propagates the hidden layer to the output to make a prediction\n",
    "```python\n",
    "layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "```\n",
    "\n",
    "Compute by how much the prediction missed\n",
    "```python\n",
    "layer_2_error = y - layer_2\n",
    "```\n",
    "\n",
    "We're going to store the derivative (mustard orange in the graphic above) in a list, holding the derivative at each timestep.\n",
    "```python\n",
    "layer_2_deltas.append((layer_2_error)*sigmoid(layer_2, True))\n",
    "```\n",
    "\n",
    "Calculate the sum of the absolute errors so that we have a scalar error (to track propagation).  \n",
    "We'll end up with a sum of the error at each binary position.\n",
    "```python\n",
    "overallError += np.abs(layer_2_error[0])\n",
    "```\n",
    "\n",
    "Rounds the output (to a binary value, since it is between 0 and 1) and stores it in the designated slot of d. \n",
    "```python\n",
    "d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "```\n",
    "\n",
    "Copies the layer_1 value into an array so that at the next time step we can apply the hidden layer at the current one.\n",
    "```python\n",
    "layer_1_values.append(copy.deepcopy(layer_1))\n",
    "```\n",
    "\n",
    "#### backpropagation\n",
    "So, we've done all the forward propagating for all the time steps, and we've computed the derivatives at the output layers and stored them in a list. Now we need to backpropagate, starting with the last timestep, backpropagating to the first\n",
    "```python\n",
    "for position in range(binary_dim):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Indexing the input data like we did before\n",
    "```python\n",
    "X = np.array([[a[position],b[position]]])\n",
    "```\n",
    "\n",
    "Selecting the current hidden layer from the list.\n",
    "```python\n",
    "layer_1 = layer_1_values[-position-1]\n",
    "```\n",
    "\n",
    "Selecting the previous hidden layer from the list\n",
    "```python\n",
    "prev_layer_1 = layer_1_values[-position-2]\n",
    "```\n",
    "\n",
    "Selecting the current output error from the list\n",
    "```python\n",
    "layer_2_delta = layer_2_deltas[-position-1]\n",
    "```\n",
    "\n",
    "This computes the current hidden layer error given the error at the hidden layer from the future and the error at the current output layer.\n",
    "```python\n",
    "layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid(layer_1, True)\n",
    "```\n",
    "\n",
    "Now that we have the derivatives backpropagated at this current time step, we can construct our weight updates (but not actually update the weights just yet). We don't actually update our weight matrices until after we've fully backpropagated everything.  \n",
    "Why? Well, we use the weight matrices for the backpropagation. Thus, we don't want to go changing them yet until the actual backprop is done.\n",
    "```python\n",
    "synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "synapse_0_update += X.T.dot(layer_1_delta)\n",
    "```\n",
    "\n",
    "Now that we've backpropped everything and created our weight updates.  \n",
    "It's time to update our weights (and empty the update variables).\n",
    "```python\n",
    "synapse_0 += synapse_0_update * alpha\n",
    "synapse_1 += synapse_1_update * alpha\n",
    "synapse_h += synapse_h_update * alpha \n",
    "\n",
    "synapse_0_update *= 0\n",
    "synapse_1_update *= 0\n",
    "synapse_h_update *= 0\n",
    "```\n",
    "\n",
    "Just some nice logging to show progress\n",
    "```python\n",
    "if(j % 1000 == 0):\n",
    "    print(\"Error:\" + str(overallError))\n",
    "    print(\"Pred:\" + str(d))\n",
    "    print(\"True:\" + str(c))\n",
    "    out = 0\n",
    "    for index,x in enumerate(reversed(d)):\n",
    "        out += x*pow(2,index)\n",
    "    print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "    print(\"------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Anyone Can Learn To Code an LSTM-RNN in Python](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
