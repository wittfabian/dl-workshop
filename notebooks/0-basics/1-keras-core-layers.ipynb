{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overiew Keras Core Layers\n",
    "[Keras Core Layer Documentation](https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense\n",
    "\n",
    "A dense layer is a standard fully connected NN layer, let’s start with some sample source code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "d = Dense(3, input_dim=3, kernel_initializer='uniform', activation='linear')\n",
    "d.set_weights([np.array([[.1, .2, .5], [.1, .2, .5], [.1, .2, .5]]), np.array([0, 0, 0])])\n",
    "print_out(d, [[10, 20, 30]])\n",
    "# [[  6.  12.  30.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the input [10,20,30] got converted to [6, 12, 30] using a linear activation layer and the weights [.1, .2, .5] for each input row.  \n",
    "So taking the last output node which all weights are 0.5 we get the output (30) by calculating: 10\\*0.5 + 20\\*0.5 + 30\\*0.5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "An activation function is a function that produces the layer output values by applying an arbitrary function to the input values of the layer.  \n",
    "This function should have a useful derivative as this is used during the optimisation (backward) step of training.  \n",
    "There are many standard activation functions used in NN a great visual summary of these common activation functions can be found at the bottom of the [Activation Function Wikipedia page](https://en.wikipedia.org/wiki/Activation_function). \n",
    "\n",
    "Partially reproduced here for convenience:\n",
    "\n",
    "![Activation function (1)](../images/tbl1.png \"Activation function\")\n",
    "![Activation function (2)](../images/tbl2.png \"Activation function\")\n",
    "![Activation function (3)](../images/tbl3.png \"Activation function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function specified in this layer is applied to each input element individually (element wise) so input data dimensions can be arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print_out(Activation('tanh'), [.5, 1, 2, 3])\n",
    "# [ 0.46211714, 0.76159418, 0.96402758, 0.99505478 ]\n",
    "\n",
    "print_out(Activation('softplus'), [.5, 1, 2, 3])\n",
    "# [ 0.97407699, 1.31326163, 2.12692809, 3.04858732 ]\n",
    "\n",
    "print_out(Activation('relu'), [-2, -1, 0, 1, 2])\n",
    "# [ 0., 0., 0., 1., 2.]\n",
    "\n",
    "print_out(Activation('sigmoid'), [.5, 1, 2, 3])\n",
    "# [ 0.62245935, 0.7310586,  0.88079709, 0.95257413]\n",
    "\n",
    "print_out(Activation('hard_sigmoid'), [.5, 1, 2, 3])\n",
    "# [ 0.60000002, 0.69999999, 0.89999998, 1. ]\n",
    "\n",
    "print_out(Activation('linear'), [.5, 1, 2, 3])\n",
    "# [0.5, 1., 2., 3. ] – no weights set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeDistributedDense\n",
    "\n",
    "A very similar layer to the standard Dense layer with the exception that we are now working with an additional time dimension.  \n",
    "So the input and output are in the shape: (nb_sample, time_dimension, input_dim).  \n",
    "So reproducing the Dense example we get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "d = TimeDistributedDense(3, init='uniform', activation='linear', input_dim=3)\n",
    "d.set_weights([np.array([[.1, .2, .5], [.1, .2, .5], [.1, .2, .5]]), np.array([0, 0, 0])])\n",
    "print_out(d, [[[10, 20, 30]]])\n",
    "# [[[  6.  12.  30.]]]\n",
    "# [0.5, 1., 2., 3. ] – no weights set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout layers are used to reduce overfitting by randomly turning off inputs.  \n",
    "It is important to note that Dropout only occurs during training.  During the test phase we do not turn off inputs.  \n",
    "It is also very important to note that output values propagated forward (i.e. not turned off) must increase in value to compensate for the nodes being turned off.  \n",
    "This means that the output value of the layer is the same with or without dropout.  \n",
    "The following simple example shows this a little bit more intuitively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print_out(Dropout(.3), [1, 2, 3, 4, 5])\n",
    "# [0, 0, 0, 5.71428585, 7.14285755]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with 30% dropout we see that 3 output nodes were turned off (set to 0).  \n",
    "To compensate for the output value of the layer all the other values were increased accordingly (probabilistically so they may not exactly match the output).\n",
    "\n",
    "To tune dropout layers [Hinton suggests](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) training without dropout until a good layer settings are found.  \n",
    "Then slowly increase dropout until optimal validation score is found after the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "The reshape layer reshapes input to a new shape.  \n",
    "The number of dimensions however  must remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print_out(Reshape((2, -1)), [[1, 2, 3, 4, 5, 6]])\n",
    "# [[[ 1.  2.  3.], [ 4.  5.  6.]]]\n",
    "\n",
    "print_out(Reshape((3, -1)), [[1, 2, 3, 4, 5, 6]])\n",
    "# [[[ 1.  2.],[ 3.  4.],[ 5.  6.]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "Flattens rows of a 3D matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print_out(Flatten(), [[[1, 2, 3], [4, 5, 6]]])\n",
    "# [[ 1.  2.  3.  4.  5.  6.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RepeatVector\n",
    "Copies a 2D input matrix into a 3D matrix n times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print_out(RepeatVector(RepeatVector), [[1, 2, 3]])\n",
    "# [[[ 1.  2.  3.], [ 1.  2.  3.]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://www.picnet.com.au/blogs/guido/post/2016/05/16/review-of-keras-deep-learning-core-layers/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
