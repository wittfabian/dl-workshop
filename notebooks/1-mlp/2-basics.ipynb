{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization with Gradient Descent\n",
    "\n",
    "Improving our neural network by optimizing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation allowed us to measure how each weight in the network contributed to the overall error. This ultimately allowed us to change these weights using a different algorithm, **Gradient Descent**. \n",
    "\n",
    "The takeaway here is that **backpropagation doesn't optimize**! It moves the error information from the end of the network to all the weights inside the network so that a different algorithm can optimize those weights to fit our data. We actually have a plethora of different **nonlinear optimization methods** that we could use with backpropagation:\n",
    "\n",
    "**A Few Optimization Methods:**\n",
    "* [Annealing](http://auai.org/uai2015/proceedings/papers/73.pdf)\n",
    "* [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "* [AW-SGD](https://arxiv.org/pdf/1506.09016v1.pdf)\n",
    "* [Momentum (SGD)](http://ruder.io/optimizing-gradient-descent/index.html#momentum)\n",
    "* [Nesterov Momentum (SGD)](http://proceedings.mlr.press/v28/sutskever13.pdf)\n",
    "* [AdaGrad](http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n",
    "* [AdaDelta](http://ruder.io/optimizing-gradient-descent/index.html#adadelta)\n",
    "* [ADAM](http://ruder.io/optimizing-gradient-descent/index.html#adam)\n",
    "* [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "* [LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)\n",
    "\n",
    "\n",
    "Many of these optimizations are good for different purposes, and in some cases several can be used together. In this tutorial, we will walk through Gradient Descent, which is arguably the simplest and most widely used neural network optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you had a red ball inside of a rounded bucket like in the picture below.  \n",
    "Imagine further that the red ball is trying to find the bottom of the bucket.  \n",
    "This is **optimization**. In our case, the ball is optimizing it's position (from left to right) to find the lowest point in the bucket.\n",
    "\n",
    "So, to gamify this a bit. The ball has two options, left or right. It has one goal, get as low as possible.  \n",
    "So, it needs to press the left and right buttons correctly to find the lowest spot\n",
    "\n",
    "![](../images/sgd-description-001.png)\n",
    "\n",
    "So, what information does the ball use to adjust its position to find the lowest point? The only information it has is the **slope** of the side of the bucket at its current position, pictured below with the blue line.  \n",
    "Notice that when the slope is negative (downward from left to right), the ball should move to the right. However, when the slope is positive, the ball should move to the left. As you can see, this is more than enough information to find the bottom of the bucket in a few iterations. This is a sub-field of optimization called gradient optimization. (Gradient is just a fancy word for slope or steepness). \n",
    "\n",
    "![](../images/sgd-description-002.png)\n",
    "\n",
    "**Oversimplified Gradient Descent:**\n",
    "\n",
    "* Calculate slope at current position\n",
    "* If slope is negative, move right\n",
    "* If slope is positive, move left\n",
    "* (Repeat until slope == 0)\n",
    "\n",
    "The question is, however, *how much should the ball move* at each time step? Look at the bucket again.  \n",
    "The steeper the slope, the farther the ball is from the bottom. That's helpful! \n",
    "\n",
    "Let's improve our algorithm to leverage this new information. Also, let's assume that the bucket is on an (x,y) plane.  \n",
    "So, it's location is x (along the bottom). Increasing the ball's \"x\" position moves it to the right.  \n",
    "Decreasing the ball's \"x\" position moves it to the left.\n",
    "\n",
    "**Naive Gradient Descent:**\n",
    "* Calculate \"slope\" at current \"x\" position\n",
    "* Change x by the negative of the slope. (x = x - slope)\n",
    "* (Repeat until slope == 0)\n",
    "\n",
    "This is a considerable improvement to our algorithm. For very positive slopes, we move left by a lot.  \n",
    "For only slightly positive slopes, we move left by only a little. As it gets closer and closer to the bottom, it takes smaller and smaller steps until the slope equals zero, at which point it stops. This stopping point is called convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sometimes It Breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent isn't perfect. Let's take a look at its issues and how people get around them.  \n",
    "This will allow us to improve our network to overcome these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: When slopes are too big\n",
    "\n",
    "How big is too big? Remember our step size is based on the steepness of the slope. Sometimes the slope is so steep that we overshoot by a lot. Overshooting by a little is ok, but sometimes we overshoot by so much that we're even farther away than we started! See below. \n",
    "\n",
    "![](../images/sgd-description-003.png)\n",
    "\n",
    "What makes this problem so destructive is that overshooting this far means we land at an EVEN STEEPER slope in the opposite direction. This causes us to overshoot again EVEN FARTHER. This viscious cycle of overshooting leading to more overshooting is called divergence.\n",
    "\n",
    "## Solution 1: Make Slopes Smaller\n",
    "\n",
    "Lol. This may seem too simple to be true, but it's used in pretty much every neural network. If our gradients are too big, we make them smaller! We do this by multiplying them (all of them) by a single number between 0 and 1 (such as 0.01). This fraction is typically a single float called **alpha**. When we do this, we don't overshoot and our network converges. \n",
    "\n",
    "![](../images/sgd-description-004.png)\n",
    "\n",
    "**Improved Gradient Descent:**  \n",
    "alpha = 0.1 (or some number between 0 and 1)\n",
    "* Calculate \"slope\" at current \"x\" position\n",
    "* x = x - (alpha*slope)\n",
    "* (Repeat until slope == 0)\n",
    "\n",
    "\n",
    "## Problem 2: Local Minimums\n",
    "\n",
    "Sometimes your bucket has a funny shape, and following the slope doesn't take you to the absolute lowest point.  \n",
    "Consider the picture below.\n",
    "\n",
    "![](../images/sgd-description-005.png)\n",
    "\n",
    "This is by far the most difficult problem with gradient descent. There are a myriad of options to try to overcome this. Generally speaking, they all involve an element of random searching to try lots of different parts of the bucket. \n",
    "\n",
    "## Solution 2: Multiple Random Starting States\n",
    "\n",
    "There are a myriad of ways in which randomness is used to overcome getting stuck in a local minimum. It begs the question, if we have to use randomness to find the global minimum, why are we still optimizing in the first place? Why not just try randomly? The answer lies in the graph below. \n",
    "\n",
    "![](../images/sgd-description-006.png)\n",
    "\n",
    "Imagine that we randomly placed 100 balls on this line and started optimizing all of them. If we did so, they would all end up in only 5 positions, mapped out by the five colored balls above. The colored regions represent the domain of each local minimum. For example, if a ball randomly falls within the blue domain, it will converge to the blue minimum. This means that to search the entire space, we only have to randomly find 5 spaces! This is far better than pure random searching, which has to randomly try EVERY space (which could easily be millions of places on this black line depending on the granularity). \n",
    "\n",
    "**In Neural Networks**: One way that neural networks accomplish this is by having very large hidden layers. You see, each hidden node in a layer starts out in a different random starting state. This allows each hidden node to converge to different patterns in the network. Parameterizing this size allows the neural network user to potentially try thousands (or tens of billions) of different local minima in a single neural network.\n",
    "\n",
    "**Sidenote 1: This is why neural networks are so powerful!** They have the ability to search far more of the space than they actually compute! We can search the entire black line above with (in theory) only 5 balls and a handful of iterations. Searching that same space in a brute force fashion could easily take orders of magnitude more computation.\n",
    "\n",
    "**Sidenote 2**: A close eye might ask, \"Well, why would we allow a lot of nodes to converge to the same spot? That's actually wasting computational power!\" That's an excellent point. The current state-of-the-art approaches to avoiding hidden nodes coming up with the same answer (by searching the same space) are Dropout and Drop-Connect.\n",
    "\n",
    "\n",
    "## Problem 3: When Slopes are Too Small\n",
    "\n",
    "Neural networks sometimes suffer from the slopes being too small.  \n",
    "The answer is also obvious but I wanted to mention it here to expand on its symptoms. Consider the following graph.\n",
    "\n",
    "![](../images/sgd-description-007.png)\n",
    "\n",
    "Our little red ball up there is just stuck! If your alpha is too small, this can happen. The ball just drops right into an instant local minimum and ignores the big picture. It doesn't have the **umph** to get out of the rut.\n",
    "\n",
    "![](../images/sgd-description-008.png)\n",
    "\n",
    "And perhaps the more obvious symptom of deltas that are too small is that the convergence will just take a very, very long time. \n",
    "\n",
    "## Solution 3: Increase the Alpha\n",
    "\n",
    "As you might expect, the solution to both of these symptoms is to increase the alpha. We might even multiply our deltas by a weight higher than 1. This is very rare, but it does sometimes happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point you might be wondering, how does this relate to neural networks and backpropagation? This is the hardest part, and also quite important.\n",
    "\n",
    "![](../images/sgd-description-009.png)\n",
    "\n",
    "That big nasty curve? In a neural network, we're trying to minimize the error with respect to the weights. So, what that curve represents is the network's error relative to the position of a single weight.\n",
    "\n",
    "So, if we computed the network's error for every possible value of a single weight, it would generate the curve you see above. We would then pick the value of the single weight that has the lowest error (the lowest part of the curve). Single weight because it's a two-dimensional plot. Thus, the x dimension is the value of the weight and the y dimension is the neural network's error when the weight is at that position.\n",
    "\n",
    "Let's take a look at what this process looks like in a simple 2 layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00505119]\n",
      " [ 0.00505119]\n",
      " [ 0.99494905]\n",
      " [ 0.99494905]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv is True :\n",
    "        return x*(1-x)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([[0, 1],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 0]])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0, 0, 1, 1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "synapse_0 = 2*np.random.random((2,1)) - 1\n",
    "\n",
    "for iter in range(10000):\n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "    layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    layer_1_error = layer_1 - y\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in layer_1\n",
    "    layer_1_delta = layer_1_error * sigmoid(layer_1, True)\n",
    "    synapse_0_derivative = np.dot(layer_0.T, layer_1_delta)\n",
    "\n",
    "    # update weights\n",
    "    synapse_0 -= synapse_0_derivative\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(layer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case, we have a single error at the output (single value), which is computed with `layer_1_error = layer_1 - y`. Since we have 2 weights, the output \"error plane\" is a 3 dimensional space. We can think of this as an (x,y,z) plane, where vertical is the error, and x and y are the values of our two weights in syn0.\n",
    "\n",
    "Let's try to plot what the error plane looks like for the network/dataset above. So, how do we compute the error for a given set of weights?\n",
    "\n",
    "```python\n",
    "layer_0 = X\n",
    "layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "\n",
    "layer_1_error = layer_1 - y\n",
    "```\n",
    "\n",
    "If we take that logic and plot the overall error (a single scalar representing the network error over the entire dataset) for every possible set of weights (from -10 to 10 for x and y), it looks something like this.\n",
    "\n",
    "![](../images/sgd-description-010.gif)\n",
    "\n",
    "Don't be intimidated by this. It really is as simple as computing every possible set of weights, and the error that the network generates at each set. x is the first synapse_0 weight and y is the second synapse_0 weight. z is the overall error. As you can see, our output data is **positively correlated** with the first input data. Thus, the error is minimized when x (the first synapse_0 weight) is high. What about the second synapse_0 weight? How is it optimal?\n",
    "\n",
    "#### How Our 2 Layer Neural Network Optimizes\n",
    "\n",
    "So, given code above end up computing the error. It can be natural to see that the following code optimize to reduce the error. This is where Gradient Descent is happening! Remember our pseudocode? \n",
    "\n",
    "```python\n",
    "(1) layer_1_delta = layer_1_error * sigmoid(layer_1, True)\n",
    "(2) synapse_0_derivative = np.dot(layer_0.T, layer_1_delta)\n",
    "\n",
    "(3) synapse_0 -= synapse_0_derivative\n",
    "```\n",
    "\n",
    "**Naive Gradient Descent:**\n",
    "* (1) and (2): Calculate \"slope\" at current \"x\" position\n",
    "* (3): Change x by the negative of the slope. (x = x - slope)\n",
    "* (Repeat until slope == 0)\n",
    "\n",
    "It's exactly the same thing! The only thing that has changed is that we have 2 weights that we're optimizing instead of just 1. The logic, however, is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [A Neural Network in 13 lines of Python](http://iamtrask.github.io/2015/07/27/python-network-part2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
