{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Types of Recurrent Neural Networks](../images/diags-rnn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each rectangle is a vector and arrows represent functions\n",
    "* Input vectors are in red\n",
    "* Output vectors are in blue\n",
    "* Green vectors hold the RNN's state. \n",
    "\n",
    "### From left to right: \n",
    "* **Vanilla mode of processing without RNN**, from fixed-sized input to fixed-sized output\n",
    " * e.g. image classification\n",
    "* **Sequence output**\n",
    " * e.g. image captioning takes an image and outputs a sentence of words\n",
    "* **Sequence input**\n",
    " * e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment. \n",
    "* **Sequence input and sequence output**\n",
    " * e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French\n",
    "* **Synced sequence input and output**\n",
    " * e.g. video classification where we wish to label each frame of the video). \n",
    " \n",
    "Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rnn = RNN()\n",
    "# x is an input vector, y is the RNN's output vector\n",
    "y = rnn.step(x) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class RNN:\n",
    "    # ...\n",
    "    def step(self, x):\n",
    "        # update the hidden state\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # compute the output vector\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer recurrent network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "y1 = rnn1.step(x)\n",
    "y = rnn2.step(y1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two separate RNNs:  \n",
    "One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - itâ€™s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The repeating module in a standard RNN contains a single layer.](../images/rnn-description-0012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) & [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
